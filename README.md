# UniMPR: A Unified Framework for Multimodal Place Recognition with Arbitrary Sensor Configurations

[Zhangshuo Qi](https://github.com/QiZS-BIT)<sup>1</sup>, [Jingyi Xu](https://github.com/BIT-XJY)<sup>2</sup>, [Luqi Cheng](https://github.com/ChengLuqi)<sup>1</sup>, Shichen Wen<sup>1</sup>, Yiming Ma<sup>3</sup>, [Guangming Xiong](https://ieeexplore.ieee.org/author/37286205000)<sup>1*</sup>

<sup>1</sup>Beijing Institute of Technology <sup>2</sup>Shanghai Jiao Tong University <sup>3</sup>University of New South Wales

![image](https://github.com/QiZS-BIT/LRFusionPR/blob/main/assets/motivation.png)

**UniMPR is a unified multimodal place recognition framework.** Using a single trained model, it can adapt to arbitrary modality input, 
including cameras (both single-view and multi-view), LiDAR, and radar (both single-chip and scanning radar), enabling plug-and-play deployment in different scenarios.
