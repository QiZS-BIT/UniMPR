# UniMPR: A Unified Framework for Multimodal Place Recognition with Heterogeneous Sensor Configurations

[Zhangshuo Qi](https://github.com/QiZS-BIT)<sup>1</sup>, [Jingyi Xu](https://github.com/BIT-XJY)<sup>2</sup>, [Luqi Cheng](https://github.com/ChengLuqi)<sup>1</sup>, Shichen Wen<sup>1</sup>, Yiming Ma<sup>3</sup>, [Guangming Xiong](https://ieeexplore.ieee.org/author/37286205000)<sup>1*</sup>

<sup>1</sup> Beijing Institute of Technology <sup>2</sup> Shanghai Jiao Tong University <sup>3</sup> University of New South Wales

[[arxiv]](https://arxiv.org/pdf/2512.18279)

![image](https://github.com/QiZS-BIT/UniMPR/blob/main/assets/intro.png)

**UniMPR is a unified multimodal place recognition framework.** Using a single trained model, it can adapt to arbitrary modality input, 
including cameras (both single-view and multi-view), LiDAR, and radar (both single-chip and scanning radar), enabling plug-and-play deployment in different scenarios.

**Currently, we have released the evaluation code on the nuScenes and Boreas datasets, along with our pre-trained weights.**
Please feel free to use it!

## Table of Contents
- [Installation](#installation)
- [Data Preparation](#data-preparation)
- [Evaluation](#evaluation)
- [Download](#download)
- [TODO](#todo)
- [Citation](#citation)

## Installation
* Ubuntu 20.04 + Python 3.10
* CUDA 12.1 + Pytorch 2.1.0
```
git clone https://github.com/QiZS-BIT/UniMPR.git
cd UniMPR
conda create -n unimpr python=3.10
conda activate unimpr
pip install -r requirements.txt
pip install modules/diff-gaussian-rasterization
```

## Data Preparation
### nuScenes
* Download the official [nuScenes dataset](https://www.nuscenes.org/nuscenes).
* Download the [training indexes of nuScenes-BostonSeaport](https://drive.google.com/drive/folders/15PHPzwaj3_qNSJHcrVV6brslWH7dOTLJ?usp=sharing) generated by [AutoPlace](https://github.com/ramdrop/autoplace).
* Generate the infos, indexes and polar BEVs needed to run the code.
```
cd dataset/NuScenes
python gen_info.py
python gen_index.py
python gen_bev.py
```

### Boreas
* Download the official [Boreas dataset](https://www.boreas.utias.utoronto.ca).
* Generate the infos, indexes and polar BEVs needed to run the code.
```
cd dataset/Boreas
python gen_info.py
python gen_index.py
python gen_bev.py
```

### Self-Collected Dataset
For multi-session place recognition, we recommend using a data structure similar to that of nuScenes 
and Boreas for fast deployment.

## Evaluation
The first step is to modify the ``config/params_full.py``. Set the ``self.checkpoint_path`` to the local path of the pretrained weights. 
Additionally, you can control which modalities to use as input by configuring ``self.l_enable``, ``self.c_enable``,
and ``self.r_enable``.

Next, modify ``configs/params_nusc.py`` and ``configs/params_boreas.py``. 
In these files, keep the file paths used for evaluating specific sequences, and comment out all other file paths.

Run the following command to evaluate:
```
python test_nusc.py
python test_boreas.py
```

## Download
* Our pre-trained weights are available at this [link](https://drive.google.com/file/d/1sskIHSz-oA9K1k-ME7yGFDsLTnCF0G2p/view?usp=sharing).


## TODO
- [X] Release the [paper](https://arxiv.org/abs/2504.19186)
- [X] Release the evaluation code and pretrained model
- [ ] Release the training scripts, along with evaluation code for other datasets

## Citation
If you find this project useful for your research, we would appreciate it if you could cite our paper:
```
@misc{qi2025unimpr,
      title={UniMPR: A Unified Framework for Multimodal Place Recognition with Heterogeneous Sensor Configurations}, 
      author={Zhangshuo Qi and Jingyi Xu and Luqi Cheng and Shichen Wen and Yiming Ma and Guangming Xiong},
      year={2025},
      eprint={2512.18279},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2512.18279}, 
}
```
